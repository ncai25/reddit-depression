{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncai25/reddit-depression/blob/main/Reddit_Depression_Stencil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFvbbC6VtZm"
      },
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMOTL7mV9T9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoBxKQ_OVl-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d9d158-a048-439a-b6f1-342f714178c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install gensim\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from os.path import exists\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "# from gensim.models import LdaMulticore\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = 'drive/MyDrive/Colab Notebooks/final_project/student.pkl'\n",
        "FOLDER = 'drive/MyDrive/Colab Notebooks/final_project'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "npmP2R1UclcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "outputs": [],
      "source": [
        "def load():\n",
        "    \"\"\"Load pickles\"\"\"\n",
        "    if exists(FILEPATH):\n",
        "        with open(FILEPATH, 'rb') as f:\n",
        "            data = pd.read_pickle(f)\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"File {FILEPATH} does not exist.\")\n",
        "        return None\n",
        "\n",
        "df = load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7SIeyvzTvRw"
      },
      "outputs": [],
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "outputs": [],
      "source": [
        "def dataset_generation(df, depression_subreddits):\n",
        "    \"\"\"Build control and symptom datasets\n",
        "    For the control dataset, only posts were at least 180 days earlier than their (earliest) post in a mental health subreddit.”\n",
        "    \"\"\"\n",
        "    # a sympton dataset consists of all the posts from depression subreddits\n",
        "    df_depression = df[df['subreddit'].isin(depression_subreddits)]\n",
        "\n",
        "    # a control dataset posts from non-depression subreddits of the same authors\n",
        "    authors = df_depression['author'].unique()\n",
        "    df_control = df[(df['author'].isin(authors)) & (~df['subreddit'].isin(depression_subreddits))]\n",
        "\n",
        "    # find the earliest depression posts for each author\n",
        "    grouped_author = df_depression.groupby('author')['created_utc'].min()\n",
        "    author_to_earliest = grouped_author.to_dict()\n",
        "    time_delta = 180 * 24 * 60 * 60\n",
        "\n",
        "    # filter the control posts so that they occur at least 180 days before that timestamp\n",
        "    df_control = df_control.copy()\n",
        "    df_control['earliest'] = df_control['author'].map(author_to_earliest)\n",
        "    df_control['delta'] = df_control['created_utc'] - df_control['earliest']\n",
        "    df_control = df_control[df_control['delta'] < -time_delta]\n",
        "\n",
        "    return df_depression, df_control\n",
        "\n",
        "df_depression, df_control = dataset_generation(df, depression_subreddits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests, reference (depression_df: 95916; control_df: 4369)\n",
        "print(len(df_depression))\n",
        "print(len(df_control))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9grI60SZk8i",
        "outputId": "93565fdd-0468-4155-fb75-bd828025cfc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94514\n",
            "4369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWGVUju_WxuP",
        "outputId": "4df15936-31cc-417d-fe38-70b76bead135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: happiestfuntokenizing in /usr/local/lib/python3.10/dist-packages (0.0.7)\n"
          ]
        }
      ],
      "source": [
        "# Tokenization + Stopword Removal\n",
        "\n",
        "!pip install happiestfuntokenizing\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text: str):\n",
        "    \"\"\"Tokenize a single text string into a list of tokens\"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    punctuation = set(string.punctuation)\n",
        "    tokens = [token for token in tokens if token not in punctuation]\n",
        "    return tokens\n",
        "\n",
        "# def lower_and_remove_punctuation(text: str):\n",
        "#     \"\"\"Lowercase text and remove punctuation for text column for Roberta\"\"\"\n",
        "#     text = text.lower()\n",
        "#     punctuation = set(string.punctuation)\n",
        "#     text = ''.join([char for char in text if char not in punctuation])\n",
        "#     return text\n",
        "\n",
        "def find_stopwords(control_dataset):\n",
        "    \"\"\"Find top 100 words from the control dataset to use as stop words.\"\"\"\n",
        "\n",
        "    all_tokens = []\n",
        "    for tokens in (control_dataset['tokens'].tolist()):\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    freq = Counter(all_tokens)\n",
        "    top_100_words = [word for word, _ in freq.most_common(100)]\n",
        "\n",
        "    return top_100_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed data from Google Drive\n",
        "with open(f'{FOLDER}/pre_depression.pkl', 'rb') as f:\n",
        "    pre_depression = pickle.load(f)\n",
        "\n",
        "with open(f'{FOLDER}/pre_control.pkl', 'rb') as f:\n",
        "    pre_control = pickle.load(f)"
      ],
      "metadata": {
        "id": "9g3Uqt6Kc8gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing if preprocessed data is loaded\n",
        "\n",
        "print(pre_depression['tokens'].head())\n",
        "print()\n",
        "print(pre_control['tokens'].head())\n",
        "print(pre_control['text'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF6P8qEsJDtL",
        "outputId": "4c6e3fd4-081c-4a8d-eba0-1ccbefc95571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20    [trying, hi, sorry, writing, bad, headache, di...\n",
            "39    [friend, blanking, feels, months, complete, sh...\n",
            "67    [study, hall, social, anxiety, bruh, study, ha...\n",
            "72                 [positive, thoughts, happy, publish]\n",
            "79    [starting, blowup, mattress, today, very, “, d...\n",
            "Name: tokens, dtype: object\n",
            "\n",
            "315                         [man, love, bandicoot, crash]\n",
            "651     [pc, 700-750, budget, gaming, high, ultra, set...\n",
            "730     [price, gpus, down, bitcoin, mining, trying, s...\n",
            "1354    [our, service, available, area, hey, totally, ...\n",
            "1598                                                [wow]\n",
            "Name: tokens, dtype: object\n",
            "315              Man, I do love me some Bandicoot crash. \n",
            "651     How good is this PC for my 700-750$ budget? Wa...\n",
            "730     When is the price of gpus going down? I know t...\n",
            "1354    Our service is not available in your area. Hey...\n",
            "1598                                                 Wow \n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing (only need to run once)"
      ],
      "metadata": {
        "id": "WbgzCI6icqzu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixI-uBcUqNcV"
      },
      "outputs": [],
      "source": [
        "def preprocess(depression_data, control_data):\n",
        "    \"\"\"Preprocess the depression and control datasets by tokenizing and removing stopwords\"\"\"\n",
        "    control_data = control_data.copy()\n",
        "    depression_data = depression_data.copy()\n",
        "\n",
        "    # control_data['text'] = control_data['text'].apply(lower_and_remove_punctuation)\n",
        "    # depression_data['text'] = depression_data['text'].apply(lower_and_remove_punctuation)\n",
        "\n",
        "    # tokenize\n",
        "    control_data['tokens'] = control_data['text'].apply(tokenize)\n",
        "    depression_data['tokens'] = depression_data['text'].apply(tokenize)\n",
        "\n",
        "    # stop word removal\n",
        "    stopwords = find_stopwords(control_data)\n",
        "    # print(stopwords)\n",
        "    depression_data['tokens'] = depression_data['tokens'].apply(lambda tokens: [t for t in tokens if t not in stopwords])\n",
        "    control_data['tokens'] = control_data['tokens'].apply(lambda tokens: [t for t in tokens if t not in stopwords])\n",
        "    return depression_data, control_data\n",
        "\n",
        "pre_depression, pre_control = preprocess(df_depression, df_control)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessed data to Google Drive\n",
        "with open(f'{FOLDER}/pre_depression.pkl', 'wb') as f:\n",
        "    pickle.dump(pre_depression, f)\n",
        "\n",
        "with open(f'{FOLDER}/pre_control.pkl', 'wb') as f:\n",
        "    pickle.dump(pre_control, f)"
      ],
      "metadata": {
        "id": "m8p5Y8rCadO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4I37U1SXAEZ"
      },
      "source": [
        "## Reddit Topics with LDA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LDA model\n",
        "lda_model = LdaModel.load(f'{FOLDER}/lda_model.gensim')\n",
        "dictionary = Dictionary.load(f'{FOLDER}/dictionary.gensim')\n",
        "\n",
        "# Load dictionary and corpus\n",
        "with open(f'{FOLDER}/combined_corpus.pkl', 'rb') as f:\n",
        "    combined_corpus = pickle.load(f)\n",
        "with open(f'{FOLDER}/control_corpus.pkl', 'rb') as f:\n",
        "    control_corpus = pickle.load(f)"
      ],
      "metadata": {
        "id": "GMDFNIejfPvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA model (run once)"
      ],
      "metadata": {
        "id": "_LUjJ56yf7AW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf3surfWXH-q",
        "outputId": "ad080b09-e962-4e92-da24-90909623a2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "all_tokens = pre_depression['tokens'].tolist() + pre_control['tokens'].tolist()\n",
        "# LDA should be trained on the entire dataset(all posts)\n",
        "\n",
        "# dictionary and corpus\n",
        "dictionary = Dictionary(all_tokens)\n",
        "combined_corpus = [dictionary.doc2bow(doc) for doc in all_tokens]\n",
        "# create a BoW representation for each doc/post, term-document, doc/post represented as (word_id, count)\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    combined_corpus,\n",
        "    num_topics=200,\n",
        "    id2word=dictionary,\n",
        "    passes=1,\n",
        ")\n",
        "\n",
        "control_corpus = [dictionary.doc2bow(doc) for doc in pre_control['tokens']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(list(dictionary.token2id.items())[:10]))\n",
        "print(combined_corpus[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-g84SBFfOH-",
        "outputId": "61a23853-6118-470f-b7a0-be2962b05594"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'10': 0, 'able': 1, 'air': 2, 'anxiety': 3, 'anymore': 4, 'anywhere': 5, 'backing': 6, 'bad': 7, 'black': 8, 'bottom': 9}\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 3), (20, 2), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 3), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 3), (46, 2), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 2), (69, 1), (70, 2), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 2), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (96, 1), (97, 1), (98, 1), (99, 2), (100, 3), (101, 1), (102, 1)], [(27, 1), (31, 1), (39, 1), (52, 1), (74, 1), (93, 1), (94, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 2), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 3), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LDA model, dictionary, and corpus\n",
        "lda_model.save(f'{FOLDER}/lda_model.gensim')\n",
        "dictionary.save(f'{FOLDER}/dictionary.gensim')\n",
        "with open(f'{FOLDER}/combined_corpus.pkl', 'wb') as f:\n",
        "    pickle.dump(combined_corpus, f)\n",
        "\n",
        "# Save control corpus for later use\n",
        "with open(f'{FOLDER}/control_corpus.pkl', 'wb') as f:\n",
        "    pickle.dump(control_corpus, f)"
      ],
      "metadata": {
        "id": "TcdWSzhxaFZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-97hsVXNkF"
      },
      "source": [
        "## (Distil)RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blx1SWVMXYDp",
        "outputId": "672c1e79-c60d-4acb-cbd5-b9a4b47fcf14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# TODO: My RoBERTa code!\n",
        "\n",
        "# only 6 transformer blocks (run faster)\n",
        "# extracting embeddings from the 5th layer for downstream classification\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\", output_hidden_states=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distilroberta_embeddings(sentences, tokenizer, model, device):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of sentences using DistilRoBERTa.\n",
        "\n",
        "    Returns:\n",
        "    - embeddings: NumPy array of shape (num_sentences, embedding_dim).\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    batch_size = 16 # batches otherwise ram\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_sentences,\n",
        "            return_tensors=\"pt\", # pytorch tensor\n",
        "            padding=True,\n",
        "            truncation=True, # truncate sequences > max_length\n",
        "            max_length=512 # max sequence length\n",
        "        )\n",
        "\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            hidden_states = outputs.hidden_states\n",
        "\n",
        "        # 5th layer's output for embeddings\n",
        "        selected_layer = hidden_states[5]  # shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # sentence-level representation\n",
        "        sentence_embeddings = selected_layer.mean(dim=1)  # shape: (batch_size, hidden_size)\n",
        "        # each word/token has a single embedding representation.\n",
        "        # To get the embedding for a post, average the embeddings of each token\n",
        "\n",
        "        all_embeddings.append(sentence_embeddings.cpu())\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
        "\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "Ii7W0I7BFhJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWxuF2jXtwi"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ],
      "metadata": {
        "id": "sgutO4pOFUUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "# Reference values for LDA and DistilRoBERTa\n",
        "reference_values = {\n",
        "    \"Anger\": {\"LDA\": 0.794, \"DistilRoBERTa\": 0.928},\n",
        "    \"Anhedonia\": {\"LDA\": 0.906, \"DistilRoBERTa\": 0.956},\n",
        "    \"Anxiety\": {\"LDA\": 0.837, \"DistilRoBERTa\": 0.952},\n",
        "    \"Disordered Eating\": {\"LDA\": 0.905, \"DistilRoBERTa\": 0.952},\n",
        "    \"Loneliness\": {\"LDA\": 0.806, \"DistilRoBERTa\": 0.907},\n",
        "    \"Sad Mood\": {\"LDA\": 0.788, \"DistilRoBERTa\": 0.919},\n",
        "    \"Self-Loathing\": {\"LDA\": 0.815, \"DistilRoBERTa\": 0.922},\n",
        "    \"Sleep Problems\": {\"LDA\": 0.909, \"DistilRoBERTa\": 0.956},\n",
        "    \"Somatic Complaints\": {\"LDA\": 0.880, \"DistilRoBERTa\": 0.925},\n",
        "    \"Worthlessness\": {\"LDA\": 0.700, \"DistilRoBERTa\": 0.897}\n",
        "}"
      ],
      "metadata": {
        "id": "4moPEh58nDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Symptom Dataset\n",
        "\n",
        "# Don't evaluate on Fatigue, Concentration Deficit, Suicidal Thoughts\n",
        "symptom_to_subreddits = {\n",
        "    \"Anger\": [\"Anger\"],\n",
        "    \"Anhedonia\": [\"anhedonia\", \"DeadBedrooms\"],\n",
        "    \"Anxiety\": [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
        "    # \"Concentration Deficit\": [\"DecisionMaking\", \"shouldi\"],\n",
        "    \"Disordered Eating\": [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
        "    # \"Fatigue\": [\"chronicfatigue\", \"Fatigue\"],\n",
        "    \"Loneliness\": [\"ForeverAlone\", \"lonely\"],\n",
        "    \"Sad Mood\": [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
        "    \"Self-Loathing\": [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
        "    \"Sleep Problems\": [\"insomnia\", \"sleep\"],\n",
        "    \"Somatic Complaints\": [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
        "    # \"Suicidal Thoughts\": [\"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\"],\n",
        "    \"Worthlessness\": [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "}\n",
        "\n",
        "symptom_datasets = {}\n",
        "\n",
        "for symptom, subreddits in symptom_to_subreddits.items():\n",
        "    # Filter rows where the subreddit matches one of the subreddits for this symptom\n",
        "    symptom_data = pre_depression[pre_depression['subreddit'].isin(subreddits)]\n",
        "    symptom_datasets[symptom] = symptom_data\n",
        "\n",
        "# anxiety_data = symptom_datasets[\"Anxiety\"]\n",
        "# print(len(anxiety_data))"
      ],
      "metadata": {
        "id": "dMwT6hf6FN9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA evaluation"
      ],
      "metadata": {
        "id": "7Smaexs0m89E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqYXEwYmt2w9"
      },
      "outputs": [],
      "source": [
        "def get_topic_distributions(corpus, model, num_topics):\n",
        "    distributions = []\n",
        "    for doc in corpus:\n",
        "        sparse_dist = model[doc]\n",
        "        dense_dist = np.zeros(num_topics)\n",
        "        for topic_id, prob in sparse_dist:\n",
        "            dense_dist[topic_id] = prob\n",
        "        distributions.append(dense_dist)\n",
        "    return np.array(distributions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koTBPhcDXujb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3f7b3548-11ae-4632-f369-89637091cd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for symptom: Anger\n",
            "Training AUC Scores: [0.99722616 0.99794291 0.99795207 0.99798043 0.99745202]\n",
            "Testing AUC Scores: [0.94613198 0.91483141 0.89858548 0.9302259  0.92884294]\n",
            "Mean Training AUC: 0.998\n",
            "Mean Testing AUC: 0.924\n",
            "\n",
            "\n",
            "Running for symptom: Anhedonia\n",
            "Training AUC Scores: [0.99924015 0.99925321 0.99918922 0.99921041 0.999166  ]\n",
            "Testing AUC Scores: [0.96119557 0.95406614 0.95451698 0.95592301 0.95632099]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.956\n",
            "\n",
            "\n",
            "Running for symptom: Anxiety\n",
            "Training AUC Scores: [0.99931083 0.99923988 0.99936218 0.99943825 0.99941006]\n",
            "Testing AUC Scores: [0.93869649 0.9408955  0.93631066 0.94088176 0.93437597]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.938\n",
            "\n",
            "\n",
            "Running for symptom: Disordered Eating\n",
            "Training AUC Scores: [0.99890382 0.99893785 0.99893579 0.99894196 0.99892368]\n",
            "Testing AUC Scores: [0.97520155 0.96861908 0.96244412 0.95815335 0.96508473]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.966\n",
            "\n",
            "\n",
            "Running for symptom: Loneliness\n",
            "Training AUC Scores: [0.99940227 0.99928234 0.99951952 0.99941001 0.99925026]\n",
            "Testing AUC Scores: [0.8490739  0.85121659 0.84656002 0.8382276  0.84177768]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.845\n",
            "\n",
            "\n",
            "Running for symptom: Sad Mood\n",
            "Training AUC Scores: [0.9978482  0.99826633 0.99813416 0.99801428 0.99834282]\n",
            "Testing AUC Scores: [0.84208082 0.86239325 0.83644756 0.84075213 0.85315144]\n",
            "Mean Training AUC: 0.998\n",
            "Mean Testing AUC: 0.847\n",
            "\n",
            "\n",
            "Running for symptom: Self-Loathing\n",
            "Training AUC Scores: [0.99866207 0.99853716 0.99856505 0.99835023 0.99835354]\n",
            "Testing AUC Scores: [0.87549261 0.87112367 0.86241459 0.86665727 0.8584366 ]\n",
            "Mean Training AUC: 0.998\n",
            "Mean Testing AUC: 0.867\n",
            "\n",
            "\n",
            "Running for symptom: Sleep Problems\n",
            "Training AUC Scores: [0.99930266 0.99928567 0.9992737  0.99932652 0.99934376]\n",
            "Testing AUC Scores: [0.97767956 0.98501432 0.97971026 0.97404216 0.96695989]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.977\n",
            "\n",
            "\n",
            "Running for symptom: Somatic Complaints\n",
            "Training AUC Scores: [0.99870053 0.9987696  0.99873453 0.9988588  0.99876683]\n",
            "Testing AUC Scores: [0.91849278 0.91501112 0.9083554  0.91154    0.91586327]\n",
            "Mean Training AUC: 0.999\n",
            "Mean Testing AUC: 0.914\n",
            "\n",
            "\n",
            "Running for symptom: Worthlessness\n",
            "Training AUC Scores: [0.99672734 0.99636562 0.99687409 0.99673206 0.99633622]\n",
            "Testing AUC Scores: [0.75842262 0.74408789 0.73243682 0.78567917 0.77707619]\n",
            "Mean Training AUC: 0.997\n",
            "Mean Testing AUC: 0.760\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def lda_rf_cross_validation():\n",
        "    \"\"\"\n",
        "    It runs 5-fold cross validation with random forest to evaluate my LDA performance.\n",
        "    \"\"\"\n",
        "    # TODO: Print your training and testing scores!\n",
        "    for symptom, symptom_data in symptom_datasets.items():\n",
        "        print(f\"Running for symptom: {symptom}\")\n",
        "        # --- LDA Workflow ---\n",
        "        symptom_corpus = [dictionary.doc2bow(doc) for doc in symptom_data['tokens']]\n",
        "\n",
        "        num_topics = lda_model.num_topics\n",
        "        X_symptom = get_topic_distributions(symptom_corpus, lda_model, num_topics)\n",
        "        X_control = get_topic_distributions(control_corpus, lda_model, num_topics)\n",
        "\n",
        "        X_lda = np.vstack([X_symptom, X_control])\n",
        "        y_lda = np.hstack([np.ones(X_symptom.shape[0]), np.zeros(X_control.shape[0])])\n",
        "\n",
        "        rf_classifier_lda = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "        cv_lda = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        cv_results_lda = cross_validate(rf_classifier_lda, X=X_lda, y=y_lda, cv=cv_lda, scoring='roc_auc', return_train_score=True)\n",
        "        mean_train_auc_lda = round(cv_results_lda['train_score'].mean(), 3)\n",
        "        mean_test_auc_lda = round(cv_results_lda['test_score'].mean(), 3)\n",
        "\n",
        "        print(\"Training AUC Scores:\", cv_results_lda['train_score'])\n",
        "        print(\"Testing AUC Scores:\", cv_results_lda['test_score'])\n",
        "        print(f\"Mean Training AUC: {mean_train_auc_lda:.3f}\")\n",
        "        print(f\"Mean Testing AUC: {mean_test_auc_lda:.3f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if symptom not in results:\n",
        "            results[symptom] = {\"Symptom\": symptom}\n",
        "\n",
        "        results[symptom].update({\n",
        "            \"LDA Test AUC\": mean_test_auc_lda,\n",
        "            \"LDA Ref AUC\": reference_values.get(symptom, {}).get(\"LDA\", None)\n",
        "        })\n",
        "\n",
        "lda_rf_cross_validation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DistilRoBERTa Evaluation"
      ],
      "metadata": {
        "id": "eaEJjJO6nHE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FILE = os.path.join(FOLDER, 'embeddings_cache.npz')\n",
        "\n",
        "def save_embeddings_to_cache(file_path, embeddings_dict):\n",
        "    \"\"\"Save embeddings to a .npz file.\"\"\"\n",
        "    np.savez(file_path, **embeddings_dict)\n",
        "\n",
        "def load_embeddings_from_cache(file_path):\n",
        "    \"\"\"Load embeddings from a .npz file.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        return dict(np.load(file_path, allow_pickle=True))\n",
        "    return {}"
      ],
      "metadata": {
        "id": "GDYu_zdAKPDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drb_rf_cross_validation():\n",
        "    \"\"\"\n",
        "    It runs 5-fold cross validation with random forest to evaluate my DistilRoBERTa performance.\n",
        "    \"\"\"\n",
        "    embeddings_cache = load_embeddings_from_cache(CACHE_FILE)\n",
        "    updated_cache = {}\n",
        "\n",
        "    for symptom, symptom_data in symptom_datasets.items():\n",
        "        print(f\"Running for symptom: {symptom}\")\n",
        "        # --- DistilRoBERTa Workflow ---\n",
        "\n",
        "        # generate or load embeddings for depression and control datasets\n",
        "        if symptom in embeddings_cache:\n",
        "            symptom_embeddings = embeddings_cache[symptom]\n",
        "        else:\n",
        "            symptom_embeddings = get_distilroberta_embeddings(symptom_data['text'].tolist(), tokenizer, model, device)\n",
        "            updated_cache[symptom] = symptom_embeddings\n",
        "\n",
        "        if \"control\" in embeddings_cache:\n",
        "            control_embeddings = embeddings_cache[\"control\"]\n",
        "        else:\n",
        "            control_embeddings = get_distilroberta_embeddings(pre_control['text'].tolist(), tokenizer, model, device)\n",
        "            updated_cache[\"control\"] = control_embeddings\n",
        "\n",
        "        # Create labels\n",
        "        symptom_labels = [1] * len(symptom_data)\n",
        "        control_labels = [0] * len(pre_control)\n",
        "\n",
        "        X_roberta = np.vstack([symptom_embeddings, control_embeddings])\n",
        "        y_roberta = np.array(symptom_labels + control_labels)\n",
        "        rf_classifier_roberta = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        cv_roberta = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        cv_results_roberta = cross_validate(rf_classifier_roberta, X=X_roberta, y=y_roberta, cv=cv_roberta, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "        mean_train_auc_roberta = round(cv_results_roberta['train_score'].mean(), 3)\n",
        "        mean_test_auc_roberta = round(cv_results_roberta['test_score'].mean(), 3)\n",
        "\n",
        "        print(\"DistilRoBERTa Training AUC:\", cv_results_roberta['train_score'])\n",
        "        print(\"DistilRoBERTa Testing AUC:\", cv_results_roberta['test_score'])\n",
        "        print(f\"DistilRoBERTa Mean Training AUC: {mean_train_auc_roberta:.3f}\")\n",
        "        print(f\"DistilRoBERTa Mean Testing AUC: {mean_test_auc_roberta:.3f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if symptom not in results:\n",
        "            results[symptom] = {\"Symptom\": symptom}\n",
        "        results[symptom].update({\n",
        "            \"DRB Test AUC\": mean_test_auc_roberta,\n",
        "            \"DRB Ref AUC\": reference_values.get(symptom, {}).get(\"DistilRoBERTa\", None)\n",
        "        })\n",
        "    save_embeddings_to_cache(CACHE_FILE, {**embeddings_cache, **updated_cache})\n",
        "\n",
        "drb_rf_cross_validation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoeyrdMHev6n",
        "outputId": "f8672fbc-a589-4157-80fa-4d12cfbe7037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for symptom: Anger\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.96075626 0.95707611 0.9322941  0.94363149 0.92407363]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.944\n",
            "\n",
            "\n",
            "Running for symptom: Anhedonia\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.96647164 0.94914559 0.95954764 0.9590752  0.96027029]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.959\n",
            "\n",
            "\n",
            "Running for symptom: Anxiety\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.9506506  0.96061159 0.96026363 0.95526611 0.95481852]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.956\n",
            "\n",
            "\n",
            "Running for symptom: Disordered Eating\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.96263582 0.9548989  0.94647068 0.94971045 0.95930296]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.955\n",
            "\n",
            "\n",
            "Running for symptom: Loneliness\n",
            "DistilRoBERTa Training AUC: [0.99999994 0.99999986 0.99999907 0.99999868 0.99999556]\n",
            "DistilRoBERTa Testing AUC: [0.9263132  0.91821374 0.90722245 0.90911072 0.9211615 ]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.916\n",
            "\n",
            "\n",
            "Running for symptom: Sad Mood\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.93444106 0.94311506 0.93124397 0.92646562 0.9410394 ]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.935\n",
            "\n",
            "\n",
            "Running for symptom: Self-Loathing\n",
            "DistilRoBERTa Training AUC: [1.         0.99999989 1.         0.99999996 0.99999982]\n",
            "DistilRoBERTa Testing AUC: [0.93172197 0.94345272 0.92839429 0.93430259 0.93100151]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.934\n",
            "\n",
            "\n",
            "Running for symptom: Sleep Problems\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.95333298 0.96148682 0.96474299 0.95505049 0.95448705]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.958\n",
            "\n",
            "\n",
            "Running for symptom: Somatic Complaints\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.93226824 0.93483529 0.92541451 0.93039993 0.93429522]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.931\n",
            "\n",
            "\n",
            "Running for symptom: Worthlessness\n",
            "DistilRoBERTa Training AUC: [1. 1. 1. 1. 1.]\n",
            "DistilRoBERTa Testing AUC: [0.92318232 0.90678148 0.92079975 0.9283325  0.94016714]\n",
            "DistilRoBERTa Mean Training AUC: 1.000\n",
            "DistilRoBERTa Mean Testing AUC: 0.924\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "results_df = pd.DataFrame(list(results.values()))\n",
        "\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "va9NKvddcpkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77b596b-9ca5-4b60-b0c3-e6317104743b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Symptom  LDA Test AUC  LDA Ref AUC  DRB Test AUC  DRB Ref AUC\n",
            "0               Anger         0.924        0.794         0.944        0.928\n",
            "1           Anhedonia         0.956        0.906         0.959        0.956\n",
            "2             Anxiety         0.938        0.837         0.956        0.952\n",
            "3   Disordered Eating         0.966        0.905         0.955        0.952\n",
            "4          Loneliness         0.845        0.806         0.916        0.907\n",
            "5            Sad Mood         0.847        0.788         0.935        0.919\n",
            "6       Self-Loathing         0.867        0.815         0.934        0.922\n",
            "7      Sleep Problems         0.977        0.909         0.958        0.956\n",
            "8  Somatic Complaints         0.914        0.880         0.931        0.925\n",
            "9       Worthlessness         0.760        0.700         0.924        0.897\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "rcMOTL7mV9T9",
        "npmP2R1UclcO",
        "WbgzCI6icqzu",
        "U4I37U1SXAEZ",
        "_LUjJ56yf7AW",
        "E0-97hsVXNkF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}